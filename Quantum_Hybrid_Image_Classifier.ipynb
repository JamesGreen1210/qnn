{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesGreen1210/qnn/blob/main/Quantum_Hybrid_Image_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6vHETScTG7"
      },
      "source": [
        "# Installs and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkjywQDCiqXz"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-LKP07-ckL_",
        "outputId": "ddef187f-e62d-4b48-ec40-7f450a14d8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pylatexenc\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/162.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/162.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=7cb348db08049eaa050c9b55f727263ee9dcc4a1140c87d95b40969b06beff6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc\n",
            "Successfully installed pylatexenc-2.10\n"
          ]
        }
      ],
      "source": [
        "pip install pylatexenc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Q-pjgwckJ8",
        "outputId": "090baad2-ded9-4879-f221-e3cdf2d81b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-0.46.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting qiskit-terra==0.46.0 (from qiskit)\n",
            "  Downloading qiskit_terra-0.46.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rustworkx>=0.13.0 (from qiskit-terra==0.46.0->qiskit)\n",
            "  Downloading rustworkx-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit) (1.23.5)\n",
            "Collecting ply>=3.10 (from qiskit-terra==0.46.0->qiskit)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit) (1.11.4)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit) (1.12)\n",
            "Collecting dill>=0.3 (from qiskit-terra==0.46.0->qiskit)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit) (2.8.2)\n",
            "Collecting stevedore>=3.0.0 (from qiskit-terra==0.46.0->qiskit)\n",
            "  Downloading stevedore-5.1.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting symengine>=0.11 (from qiskit-terra==0.46.0->qiskit)\n",
            "  Downloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (39.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.46.0->qiskit) (1.16.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit-terra==0.46.0->qiskit)\n",
            "  Downloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra==0.46.0->qiskit) (1.3.0)\n",
            "Installing collected packages: ply, symengine, rustworkx, pbr, dill, stevedore, qiskit-terra, qiskit\n",
            "Successfully installed dill-0.3.8 pbr-6.0.0 ply-3.11 qiskit-0.46.0 qiskit-terra-0.46.0 rustworkx-0.14.0 stevedore-5.1.0 symengine-0.11.0\n"
          ]
        }
      ],
      "source": [
        "pip install qiskit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM7iVxsJckGB",
        "outputId": "8d8dbd79-0c95-4e28-b018-1d45d2311b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qiskit-algorithms\n",
            "  Downloading qiskit_algorithms-0.2.2-py3-none-any.whl (306 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/306.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m297.0/306.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.9/306.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: qiskit>=0.44 in /usr/local/lib/python3.10/dist-packages (from qiskit-algorithms) (0.46.0)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from qiskit-algorithms) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit-algorithms) (1.23.5)\n",
            "Requirement already satisfied: qiskit-terra==0.46.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.44->qiskit-algorithms) (0.46.0)\n",
            "Requirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (0.14.0)\n",
            "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (3.11)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (5.9.5)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (1.12)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (5.1.0)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (6.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra==0.46.0->qiskit>=0.44->qiskit-algorithms) (1.3.0)\n",
            "Installing collected packages: qiskit-algorithms\n",
            "Successfully installed qiskit-algorithms-0.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install qiskit-algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc38VimVckD_",
        "outputId": "d5d9503b-365e-4e7b-f9df-fa34f743594c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qiskit-machine-learning\n",
            "  Downloading qiskit_machine_learning-0.7.1-py3-none-any.whl (96 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: qiskit>=0.44 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (0.46.0)\n",
            "Requirement already satisfied: qiskit-algorithms>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (0.2.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (1.23.5)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (1.2.2)\n",
            "Collecting fastdtw (from qiskit-machine-learning)\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (67.7.2)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (0.3.8)\n",
            "Requirement already satisfied: qiskit-terra==0.46.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.44->qiskit-machine-learning) (0.46.0)\n",
            "Requirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (0.14.0)\n",
            "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (3.11)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (5.1.0)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (4.9.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->qiskit-machine-learning) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->qiskit-machine-learning) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (6.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra==0.46.0->qiskit>=0.44->qiskit-machine-learning) (1.3.0)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=512579 sha256=4c7a5fe0acc84cc928769f126d98c418c2fa82669d14c315ee7c47856491f7e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw, qiskit-machine-learning\n",
            "Successfully installed fastdtw-0.3.4 qiskit-machine-learning-0.7.1\n"
          ]
        }
      ],
      "source": [
        "pip install qiskit-machine-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhphYMoNiuTY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG647Ft_ckB2"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.optim import LBFGS\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import (\n",
        "    Module,\n",
        "    Conv2d,\n",
        "    Linear,\n",
        "    Dropout2d,\n",
        "    NLLLoss,\n",
        "    MaxPool2d,\n",
        "    Flatten,\n",
        "    Sequential,\n",
        "    ReLU,\n",
        "    CrossEntropyLoss,\n",
        "    MSELoss\n",
        ")\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit import Parameter\n",
        "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
        "from qiskit_machine_learning.connectors import TorchConnector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wTc3Xswcj-U"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKxeDT0lcj8b"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrYxGv6hcj6N",
        "outputId": "2413244b-b558-4ea2-8e20-ddb25a015afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 118976012.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 8245845.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 4422102/4422102 [00:00<00:00, 68905763.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5748742.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Getting Dataset\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY6c3ieoi7z5"
      },
      "source": [
        "# Defining DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZYcdunGjBzP"
      },
      "source": [
        "## Version 1 DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlYXm464jEWb"
      },
      "outputs": [],
      "source": [
        "# Dataloader\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R9SLgN0eHAP"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3e5PUkUfyNS"
      },
      "source": [
        "## eval_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0abEVwTf0E9"
      },
      "outputs": [],
      "source": [
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Make predictions with the model\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Accumulate the loss and accuracy values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y,\n",
        "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6LK8mifn2E"
      },
      "source": [
        "## make_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe4bVoh7fqDU"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
        "\n",
        "            # Forward pass (model outputs raw logit)\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Get prediction probability (logit -> prediction probability)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n",
        "\n",
        "            # Get pred_prob off GPU for further calculations\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn list into a tensor\n",
        "    return torch.stack(pred_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcnClE2fH9F"
      },
      "source": [
        "## train_step(), test_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc1SCl2cfNgZ"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    model.to(device)\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\\n\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.to(device)\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "\n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwQfkmZPc_bo"
      },
      "source": [
        "## Accuracy Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VItFDttcc_Yd"
      },
      "outputs": [],
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Truth labels for predictions.\n",
        "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
        "\n",
        "    Returns:\n",
        "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
        "    \"\"\"\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6QiOFO9dVg6"
      },
      "source": [
        "## create_qnn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqofqQq-dZks"
      },
      "outputs": [],
      "source": [
        "# Define and create QNN\n",
        "def create_qnn():\n",
        "    feature_map = ZZFeatureMap(2)\n",
        "    ansatz = RealAmplitudes(2, reps=1)\n",
        "    qc = QuantumCircuit(2)\n",
        "    qc.compose(feature_map, inplace=True)\n",
        "    qc.compose(ansatz, inplace=True)\n",
        "\n",
        "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
        "    qnn = EstimatorQNN(\n",
        "        circuit=qc,\n",
        "        input_params=feature_map.parameters,\n",
        "        weight_params=ansatz.parameters,\n",
        "        input_gradients=True,\n",
        "    )\n",
        "    return qnn\n",
        "\n",
        "\n",
        "qnn4 = create_qnn()\n",
        "qnn10 = create_qnn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqAjmT_Zd-EZ"
      },
      "source": [
        "## Timer Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR_Dpy3Sc_Kn"
      },
      "outputs": [],
      "source": [
        "# Shows the training time of the model\n",
        "# -----------------------------------------------\n",
        "def show_train_time(start: float, end: float, device: torch.device = None):\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.2f} seconds\")\n",
        "    return total_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci6c9bqDc_VN"
      },
      "source": [
        "# Making a Hybrid Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul40Jzflc_TH"
      },
      "source": [
        "## The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zW6h_koc_QU"
      },
      "outputs": [],
      "source": [
        "qnn = create_qnn()\n",
        "\n",
        "class FashionHybridModel(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int, qnn):\n",
        "        super().__init__()\n",
        "        # Classical Part\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # QNN Input\n",
        "        self.to_qnn_dim = nn.Linear(hidden_units*7*7, 2)\n",
        "\n",
        "        self.qnn = TorchConnector(qnn)\n",
        "\n",
        "        # Final layer after QNN\n",
        "        self.fc_after_qnn = nn.Linear(1, output_shape)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = x.view(x.size(0), -1)  # Need to flatten\n",
        "        x = F.relu(self.to_qnn_dim(x))\n",
        "\n",
        "        x = self.qnn(x)  # Apply QNN\n",
        "\n",
        "        x = self.fc_after_qnn(x)\n",
        "        return x\n",
        "\n",
        "# Model\n",
        "model_hybrid = FashionHybridModel(input_shape=1, hidden_units=10, output_shape=len(train_data.classes), qnn=qnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQmioZYehfEu"
      },
      "source": [
        "## Optimizer and Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV8aVbjuhiMg"
      },
      "outputs": [],
      "source": [
        "loss_fn_cel = nn.CrossEntropyLoss()\n",
        "optimizer_sgd = torch.optim.SGD(params=model_hybrid.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPUo5959c_Mw"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "52RXwKe_c_Hl",
        "outputId": "ffd57adb-11de-4306-f6b9-6a8a76243885"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "# Training the model (Version 1)\n",
        "# -----------------------------------------\n",
        "train_start = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 5\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"\\nEpoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_hybrid,\n",
        "        loss_fn=loss_fn_cel,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        optimizer=optimizer_sgd,\n",
        "        device=device\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_hybrid,\n",
        "        loss_fn=loss_fn_cel,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "train_end = timer()\n",
        "total_train_time = show_train_time(start=train_start,\n",
        "                                           end=train_end,\n",
        "                                           device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HIYtyosc_FM"
      },
      "outputs": [],
      "source": [
        "# Training the model (Version 2)\n",
        "# -----------------------------------------\n",
        "model_hybrid.to(device)\n",
        "optimizer_adam = optim.Adam(model_hybrid.parameters(), lr=0.001)\n",
        "loss_func_nllloss = NLLLoss()\n",
        "\n",
        "epochs = 10\n",
        "loss_list = [\\\n",
        "model_hybrid.train()  # Set model to training mode\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)  # Move data and target to the same device as model\n",
        "        optimizer_adam.zero_grad(set_to_none=True)  # Initialize gradient\n",
        "        output = model_hybrid(data)  # Forward pass\n",
        "        loss = loss_func_nllloss(output, target)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer_adam.step()  # Optimize weights\n",
        "        total_loss.append(loss.item())  # Store loss\n",
        "    loss_list.append(sum(total_loss) / len(total_loss))\n",
        "    print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waDGetKEc_Cf"
      },
      "source": [
        "## Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAQ-PQ1qeyH3"
      },
      "outputs": [],
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "model_hybrid.to(device)\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_hybrid.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_hybrid(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l2pnONWeyGP"
      },
      "source": [
        "## Evalutating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fWuzSrieyEI"
      },
      "outputs": [],
      "source": [
        "from torch import no_grad\n",
        "\n",
        "model_hybrid.to(device)\n",
        "model_hybrid.eval()  # set model to evaluation mode\n",
        "\n",
        "total_loss = []  # Initialize total_loss\n",
        "correct = 0\n",
        "total = 0  # To keep track of the total number of samples\n",
        "\n",
        "with no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data, target = data.to(device), target.to(device)  # Correctly move to device\n",
        "\n",
        "        output = model_hybrid(data)  # Calculate output correctly\n",
        "        if len(output.shape) == 1:\n",
        "            output = output.reshape(1, *output.shape)\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # Prediction\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total += target.size(0)  # Update total count\n",
        "\n",
        "        loss = loss_func(output, target)  # Calculate loss\n",
        "        total_loss.append(loss.item())  # Accumulate loss\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = sum(total_loss) / len(total_loss)\n",
        "    accuracy = correct / total * 100  # Update accuracy calculation\n",
        "\n",
        "    print(\n",
        "        \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
        "            avg_loss, accuracy\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAP8sj4AeyBs"
      },
      "source": [
        "# Classical Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iEFADhFex_S"
      },
      "source": [
        "## Making the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxdydD3Dex55"
      },
      "outputs": [],
      "source": [
        "class fashion_model_0(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2)\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "model_0 = fashion_model_0(input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "model_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDWWMi9amhIa"
      },
      "outputs": [],
      "source": [
        "train_start = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 5\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"\\nEpoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_0,\n",
        "        loss_fn=loss_fn_cel,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        optimizer=optimizer_sgd,\n",
        "        device=device\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_0,\n",
        "        loss_fn=loss_fn_cel,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "train_end = timer()\n",
        "total_train_time = show_train_time(start=train_start,\n",
        "                                           end=train_end,\n",
        "                                           device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8v09Jq_mw0D"
      },
      "outputs": [],
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_0(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sxBxhuwoQts"
      },
      "outputs": [],
      "source": [
        "# Calculate model 0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn_cel, accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_0_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je0lVhpZoWyA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOxkjBFUBCQc053838Vavmr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}